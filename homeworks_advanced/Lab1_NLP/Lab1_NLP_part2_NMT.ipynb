{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztcPpl6hPsAU"
      },
      "source": [
        "# Lab 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6raI4E_fPsAW"
      },
      "source": [
        "## Part 2: Neural Machine Translation in the wild\n",
        "In the third homework you are supposed to get the best translation you can for the EN-RU translation task.\n",
        "\n",
        "Basic approach using RNNs as encoder and decoder is implemented for you. \n",
        "\n",
        "Your ultimate task is to use the techniques we've covered, e.g.\n",
        "\n",
        "* Optimization enhancements (e.g. learning rate decay)\n",
        "\n",
        "* CNN encoder (with or without positional encoding)\n",
        "\n",
        "* attention/self-attention mechanism\n",
        "\n",
        "* pretraining the language model\n",
        "\n",
        "* [Byte Pair Encoding](https://github.com/rsennrich/subword-nmt)\n",
        "\n",
        "* or just fine-tunning BERT ;)\n",
        "\n",
        "to improve the translation quality. \n",
        "\n",
        "__Please use at least three different approaches/models and compare them (translation quality/complexity/training and evaluation time).__\n",
        "\n",
        "Write down some summary on your experiments and illustrate it with convergence plots/metrics and your thoughts. Just like you would approach a real problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORBsRFA-PsAY",
        "outputId": "15edbc7a-ade8-4487-f3b0-5c2e65def385"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-08 23:57:23--  https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/datasets/Machine_translation_EN_RU/data.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12905334 (12M) [text/plain]\n",
            "Saving to: ‘data.txt’\n",
            "\n",
            "data.txt            100%[===================>]  12.31M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2022-06-08 23:57:24 (100 MB/s) - ‘data.txt’ saved [12905334/12905334]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Thanks to YSDA NLP course team for the data\n",
        "# (who thanks tilda and deephack teams for the data in their turn)\n",
        "!wget -nc https://raw.githubusercontent.com/girafe-ai/ml-mipt/master/datasets/Machine_translation_EN_RU/data.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ihZ_nVUmsyC"
      },
      "source": [
        "The `data.txt` is a tsv file, each line of which contains a sentence in english and a corresponding translation, separated by `\\t`. We'll load it into memory and create a list of pairs, which would yield the same interface as with the torchtext's datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wzRffwfgizj",
        "outputId": "cb9963c5-e424-438b-cf22-f9fab521adc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size 50,000\n",
            "Sample:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Cordelia Hotel is situated in Tbilisi, a 3-minute walk away from Saint Trinity Church.',\n",
              "  'Отель Cordelia расположен в Тбилиси, в 3 минутах ходьбы от Свято-Троицкого собора.'],\n",
              " ['At Tupirmarka Lodge you will find a 24-hour front desk, room service, and a snack bar.',\n",
              "  'В числе удобств лоджа Tupirmarka круглосуточная стойка регистрации и снэк-бар. Гости могут воспользоваться услугой доставки еды и напитков в номер.'],\n",
              " ['Featuring free WiFi in all areas, Naigao Xiaowo offers accommodation in Shanghai.',\n",
              "  'Апартаменты Naigao Xiaowo расположены в городе Шанхай. К услугам гостей бесплатный Wi-Fi во всех зонах.'],\n",
              " ['Each has a TV and a private bathroom with shower.',\n",
              "  'В вашем распоряжении также телевизор и собственная ванная комната с душем.'],\n",
              " ['Your room comes with air conditioning and satellite TV.',\n",
              "  'Номер оснащен кондиционером и спутниковым телевидением.']]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "with open(\"data.txt\") as f:\n",
        "    data = [l.rstrip().split(\"\\t\") for l in f]\n",
        "\n",
        "print(f\"Dataset size {len(data):,}\")\n",
        "print(\"Sample:\")\n",
        "data[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjDuhDOWPsAa"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RW41CAd1ofXD"
      },
      "source": [
        "First of all, let's split our dataset into train, test and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXYV0vWaolpm",
        "outputId": "1731ebde-efa9-4467-f1b3-258b540c9932"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 40000\n",
            "Test size: 7500\n",
            "Val size: 2500\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import random_split\n",
        "\n",
        "\n",
        "data_size = len(data)\n",
        "train_size = int(0.8 * data_size)\n",
        "test_size = int(0.15 * data_size)\n",
        "val_size = data_size - train_size - test_size\n",
        "train_data, test_data, val_data = random_split(\n",
        "    data, [train_size, test_size, val_size], generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "print(f\"Train size: {len(train_data)}\")\n",
        "print(f\"Test size: {len(test_data)}\")\n",
        "print(f\"Val size: {len(val_data)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gGXkxD5odoA"
      },
      "source": [
        "Here comes the preprocessing. If you find pieces, that you don't understand, please, go back to 3rd week's practice notebook. The code is mostly taken from it.\n",
        "\n",
        "Do not hesitate to use BPE or more complex preprocessing pipeline ;)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6sGsE2moPsAa"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "\n",
        "tokenizer = WordPunctTokenizer()\n",
        "\n",
        "\n",
        "def tokenize(sent):\n",
        "    return tokenizer.tokenize(sent.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LmshoKhRpQVj"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "from torchtext.vocab import vocab as Vocab\n",
        "\n",
        "\n",
        "src_counter = Counter()\n",
        "trg_counter = Counter()\n",
        "for src, trg in train_data:\n",
        "    src_counter.update(tokenize(src))\n",
        "    trg_counter.update(tokenize(trg))\n",
        "\n",
        "src_vocab = Vocab(src_counter, min_freq=3)\n",
        "trg_vocab = Vocab(trg_counter, min_freq=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RxBsel4JpaTP",
        "outputId": "6f4f6eb9-7021-4e74-d9c0-cffd1d5db060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Source (en) vocabulary size: 6711\n",
            "Target (ru) vocabulary size: 9310\n"
          ]
        }
      ],
      "source": [
        "unk_token = \"<unk>\"\n",
        "sos_token, eos_token, pad_token = \"<sos>\", \"<eos>\", \"<pad>\"\n",
        "specials = [sos_token, eos_token, pad_token]\n",
        "\n",
        "for vocab in [src_vocab, trg_vocab]:\n",
        "    if unk_token not in vocab:\n",
        "        vocab.insert_token(unk_token, index=0)\n",
        "        vocab.set_default_index(0)\n",
        "\n",
        "    for token in specials:\n",
        "        if token not in vocab:\n",
        "            vocab.append_token(token)\n",
        "\n",
        "print(f\"Source (en) vocabulary size: {len(src_vocab)}\")\n",
        "print(f\"Target (ru) vocabulary size: {len(trg_vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "DpFjrwNHpiZq"
      },
      "outputs": [],
      "source": [
        "def encode(sent, vocab):\n",
        "    tokenized = [sos_token] + tokenize(sent) + [eos_token]\n",
        "    return [vocab[tok] for tok in tokenized]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpWUg9RuqGXm",
        "outputId": "abb36a43-a990-4d50-f07a-c9b8b85b573c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([59, 256]), torch.Size([51, 256]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "def collate_batch(batch):\n",
        "    src_list, trg_list = [], []\n",
        "    for src, trg in batch:\n",
        "        src_encoded = encode(src, src_vocab)\n",
        "        src_list.append(torch.tensor(src_encoded))\n",
        "\n",
        "        trg_encoded = encode(trg, trg_vocab)\n",
        "        trg_list.append(torch.tensor(trg_encoded))\n",
        "\n",
        "    src_padded = pad_sequence(src_list, padding_value=src_vocab[pad_token])\n",
        "    trg_padded = pad_sequence(trg_list, padding_value=trg_vocab[pad_token])\n",
        "    return src_padded, trg_padded\n",
        "\n",
        "\n",
        "batch_size = 256\n",
        "train_dataloader = DataLoader(train_data, batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "val_dataloader = DataLoader(val_data, batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_data, batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "src_batch, trg_batch = next(iter(train_dataloader))\n",
        "src_batch.shape, trg_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nvNQhzfPsAe"
      },
      "source": [
        "## Model side"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wKXxEXwFtMPP"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_tokens, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_tokens = n_tokens\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        embedded = self.dropout(embedded)\n",
        "        _, hidden = self.rnn(embedded)\n",
        "        return hidden\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_tokens, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_tokens = n_tokens\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.embedding = nn.Embedding(n_tokens, emb_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hid_dim, n_tokens)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        input = input.unsqueeze(dim=0)\n",
        "        embedded = self.embedding(input)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        pred = self.out(output.squeeze(dim=0))\n",
        "        return pred, hidden\n",
        "\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "        assert encoder.hid_dim == decoder.hid_dim, \"encoder and decoder must have same hidden dim\"\n",
        "        assert (\n",
        "            encoder.n_layers == decoder.n_layers\n",
        "        ), \"encoder and decoder must have equal number of layers\"\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        trg_len, batch_size = trg.shape\n",
        "        preds = []\n",
        "        hidden = self.encoder(src)\n",
        "\n",
        "        # First input to the decoder is the <sos> token.\n",
        "        input = trg[0, :]\n",
        "        for i in range(1, trg_len):\n",
        "            pred, hidden = self.decoder(input, hidden)\n",
        "            preds.append(pred)\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            _, top_pred = pred.max(dim=1)\n",
        "            input = trg[i, :] if teacher_force else top_pred\n",
        "\n",
        "        return torch.stack(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jQ3w4pdDPsAf"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "enc = Encoder(len(src_vocab), emb_dim=256, hid_dim=512, n_layers=4, dropout=0.5)\n",
        "dec = Decoder(len(trg_vocab), emb_dim=256, hid_dim=512, n_layers=4, dropout=0.5)\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "4_-hFxcyPsAf"
      },
      "outputs": [],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param, -0.08, 0.08)\n",
        "\n",
        "\n",
        "model.apply(init_weights);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15zD2R7cPsAg",
        "outputId": "d0b0d241-00a9-4416-c886-1e248a318bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 24,638,814 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "print(f\"The model has {count_parameters(model):,} trainable parameters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cG45OrPDPsAg"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_vocab[pad_token])\n",
        "loss_history, train_loss_history, val_loss_history = [], [], []"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_decay_step = 0.95"
      ],
      "metadata": {
        "id": "Dnz0JK9K6LSP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "sFsx5PcCudiG",
        "outputId": "64e141d8-f1e1-4487-ec9e-bd55d53110c9"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-435bf261e98b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f64cc68d1302>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f64cc68d1302>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hidden)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "\n",
        "# Please don't use tensorboard here.\n",
        "# It doesn't save the training plots in the notebook.\n",
        "n_epochs = 35\n",
        "clip = 1\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for src, trg in train_dataloader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "        output = model(src, trg)\n",
        "\n",
        "        output = output.view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if len(loss_history) % 10 == 0:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            plt.figure(figsize=(15, 5))\n",
        "\n",
        "            plt.subplot(121)\n",
        "            plt.plot(loss_history)\n",
        "            plt.xlabel(\"step\")\n",
        "\n",
        "            plt.subplot(122)\n",
        "            plt.plot(train_loss_history, label=\"train loss\")\n",
        "            plt.plot(val_loss_history, label=\"val loss\")\n",
        "            plt.xlabel(\"epoch\")\n",
        "            plt.legend()\n",
        "\n",
        "            plt.show()\n",
        "\n",
        "    train_loss /= len(train_dataloader)\n",
        "    train_loss_history.append(train_loss)\n",
        "\n",
        "    optimizer.param_groups[0]['lr'] *= learning_decay_step\n",
        "    \n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in val_dataloader:\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "            output = model(src, trg)\n",
        "\n",
        "            output = output.view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_dataloader)\n",
        "    val_loss_history.append(val_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cp3-T3dDPsAg"
      },
      "source": [
        "## Model evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtYccOGpPsAh"
      },
      "outputs": [],
      "source": [
        "trg_itos = trg_vocab.get_itos()\n",
        "model.eval()\n",
        "max_len = 50\n",
        "with torch.no_grad():\n",
        "    for i, (src, trg) in enumerate(val_data):\n",
        "        encoded = encode(src, src_vocab)[::-1]\n",
        "        encoded = torch.tensor(encoded)[:, None].to(device)\n",
        "        hidden = model.encoder(encoded)\n",
        "\n",
        "        pred_tokens = [trg_vocab[sos_token]]\n",
        "        for _ in range(max_len):\n",
        "            decoder_input = torch.tensor([pred_tokens[-1]]).to(device)\n",
        "            pred, hidden = model.decoder(decoder_input, hidden)\n",
        "            _, pred_token = pred.max(dim=1)\n",
        "            if pred_token == trg_vocab[eos_token]:\n",
        "                # Don't add it to prediction for cleaner output.\n",
        "                break\n",
        "\n",
        "            pred_tokens.append(pred_token.item())\n",
        "\n",
        "        print(f\"src: '{src.rstrip().lower()}'\")\n",
        "        print(f\"trg: '{trg.rstrip().lower()}'\")\n",
        "        print(f\"pred: '{' '.join(trg_itos[i] for i in pred_tokens[1:])}'\")\n",
        "        print()\n",
        "\n",
        "        if i == 10:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJsWrg5Z1phB"
      },
      "source": [
        "The metric often used in NMT is the BLEU. We'll also use it to evaluate our models. In fact, the goal of this homework is to beat the specified baseline BLEU scores.\n",
        "\n",
        "Here is how you can calculate the score for your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hogEKcmOPsAh",
        "outputId": "a6c098fd-9f02-4f84-99b3-2cd4693fcbb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your model shows test BLEU of 19.9\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "\n",
        "references, hypotheses = [], []\n",
        "with torch.no_grad():\n",
        "    for src, trg in test_dataloader:\n",
        "        output = model(src.to(device), trg.to(device), teacher_forcing_ratio=0)\n",
        "        output = output.cpu().numpy().argmax(axis=2)\n",
        "\n",
        "        for i in range(trg.shape[1]):\n",
        "            reference = trg[:, i]\n",
        "            reference_tokens = [trg_itos[id_] for id_ in reference]\n",
        "            reference_tokens = [tok for tok in reference_tokens if tok not in specials]\n",
        "            references.append(reference_tokens)\n",
        "\n",
        "            hypothesis = output[:, i]\n",
        "            hypothesis_tokens = [trg_itos[id_] for id_ in hypothesis]\n",
        "            hypothesis_tokens = [tok for tok in hypothesis_tokens if tok not in specials]\n",
        "            hypotheses.append(hypothesis_tokens)\n",
        "\n",
        "# corpus_bleu works with multiple references\n",
        "bleu = corpus_bleu([[ref] for ref in references], hypotheses)\n",
        "print(f\"Your model shows test BLEU of {100 * bleu:.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9hHhduLPsAh"
      },
      "source": [
        "Baseline solution BLEU score is quite low. Try to achieve at least __24__ BLEU on the test set. \n",
        "The checkpoints are:\n",
        "\n",
        "* __22__ - minimal score to submit the homework, 30% of points\n",
        "\n",
        "* __27__ - good score, 70% of points\n",
        "\n",
        "* __29__ - excellent score, 100% of points"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "anaconda-cloud": {},
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Lab1_NLP_part2_NMT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}